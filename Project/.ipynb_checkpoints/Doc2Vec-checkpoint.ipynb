{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Some useful package imports\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "token = TweetTokenizer()\n",
    "stemmer = PorterStemmer() \n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe contains  219242  rows, which consist of reviews from  7  different books\n",
      "\n",
      "Paula_Hawkins-The-Girl-On-The-Train  has  37139  reviews\n",
      "Andy-Weir-The-Martian  has  22571  reviews\n",
      "Donna-Tartt-The-Goldfinch  has  22861  reviews\n",
      "EL-James-Fifty-Shades-of-Grey  has  32977  reviews\n",
      "Fillian_Flynn-Gone_Girl  has  41974  reviews\n",
      "John-Green-The-Fault-in-our-Stars  has  35844  reviews\n",
      "Laura-Hillenbrand-Unbroken  has  25876  reviews\n"
     ]
    }
   ],
   "source": [
    "# Load the first book\n",
    "data = pd.read_csv('Paula_Hawkins-The-Girl-On-The-Train.csv', sep = '\\t', header = None)\n",
    "# Rename columns\n",
    "data.columns = ['rating', 'tail', 'title', 'review']\n",
    "data['book'] = 'Paula_Hawkins-The-Girl-On-The-Train'\n",
    "\n",
    "# Load the second book\n",
    "data1 = pd.read_csv('Andy-Weir-The-Martian.csv', sep = '\\t', header = None)\n",
    "# Rename columns\n",
    "data1.columns = ['rating', 'tail', 'title', 'review']\n",
    "data1['book'] = 'Andy-Weir-The-Martian'\n",
    "\n",
    "# Concatenate into one dataframe\n",
    "data = pd.concat((data, data1))\n",
    "# Reset index and remove the index column\n",
    "data = data.reset_index().ix[:, 1:]\n",
    "\n",
    "\n",
    "############## Concatenate the other books ########################\n",
    "\n",
    "data1 = pd.read_csv('Donna-Tartt-The-Goldfinch.csv', sep = '\\t', header = None)\n",
    "data1.columns = ['rating', 'tail', 'title', 'review']\n",
    "data1['book'] = 'Donna-Tartt-The-Goldfinch'\n",
    "\n",
    "data = pd.concat((data, data1))\n",
    "data = data.reset_index().ix[:, 1:]\n",
    "\n",
    "data1 = pd.read_csv('EL-James-Fifty-Shades-of-Grey.csv', sep = '\\t', header = None)\n",
    "data1.columns = ['rating', 'tail', 'title', 'review']\n",
    "data1['book'] = 'EL-James-Fifty-Shades-of-Grey'\n",
    "\n",
    "data = pd.concat((data, data1))\n",
    "data = data.reset_index().ix[:, 1:]\n",
    "\n",
    "data1 = pd.read_csv('Fillian_Flynn-Gone_Girl.csv', sep = '\\t', header = None)\n",
    "data1.columns = ['rating', 'tail', 'title', 'review']\n",
    "data1['book'] = 'Fillian_Flynn-Gone_Girl'\n",
    "\n",
    "data = pd.concat((data, data1))\n",
    "data = data.reset_index().ix[:, 1:]\n",
    "\n",
    "data1 = pd.read_csv('John-Green-The-Fault-in-our-Stars.csv', sep = '\\t', header = None)\n",
    "data1.columns = ['rating', 'tail', 'title', 'review']\n",
    "data1['book'] = 'John-Green-The-Fault-in-our-Stars'\n",
    "\n",
    "data = pd.concat((data, data1))\n",
    "data = data.reset_index().ix[:, 1:]\n",
    "\n",
    "data1 = pd.read_csv('Laura-Hillenbrand-Unbroken.csv', sep = '\\t', header = None)\n",
    "data1.columns = ['rating', 'tail', 'title', 'review']\n",
    "data1['book'] = 'Laura-Hillenbrand-Unbroken'\n",
    "\n",
    "##########\" Final concatenation ##############\"\n",
    "data = pd.concat((data, data1))\n",
    "data = data.reset_index().ix[:, 1:]\n",
    "\n",
    "print('The dataframe contains ', data.shape[0], ' rows, which consist of reviews from ', len(data.book.unique()), \n",
    "      ' different books\\n')\n",
    "\n",
    "for book in data.book.unique():\n",
    "    print(book, ' has ', len(data[data.book == book]), ' reviews')\n",
    "    \n",
    "    \n",
    "def process(x):\n",
    "    '''\n",
    "    Function to clear the html tags, then remove non-letters characters, \n",
    "    lowercase, remove stopwords, then join the resulting as a full text\n",
    "    '''\n",
    "\n",
    "    # Clean html tags\n",
    "    review = BeautifulSoup(x, \"html.parser\").get_text() \n",
    "    \n",
    "    # Remove non letters     \n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", review) \n",
    "    \n",
    "    # Lowercase and split to words\n",
    "    words = letters.lower().split()                             \n",
    "    \n",
    "    # It appears that searching in a set in Python is much more efficient than searching in a list\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = [w for w in words if not w in stops]   \n",
    "    \n",
    "    # Join to produce the cleaned texts \n",
    "\n",
    "    return( \" \".join( text ))\n",
    "\n",
    "data['processed'] = data['review'].apply(lambda x: process(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rachel woman considers worthless feels women valued two things looks role mother barren rather plain looking unbeknownst landlady lost job continues ride commuter train twice day unfortunately must pass home ex husband tom new wife anna recently child something rachel unable produce married moved new family home rachel shared tom posted picture newborn facebook caption never happier rachel despondency taken drinking point blackouts forgets drunk calls husband many times night even shows home signal malfunction often finds rail car stopped tracks next former home starts notice another couple live doors refers golden couple manufactures narrative lives observes day gradually become important megan golden couple disappears rachel finds integral character police investigation seen stalking neighborhood night disappearance wounds body explained megan anna look enough alike police feel may mistaken identity involved book told three voices rachel megan anna fact rachel history drunken blackouts hard time separating fact fiction makes overtly suspect even megan plenty secrets anna perfect second wife appears understand option book picked dreamworks think better hitchcock style mystery voyeuristic observation since rear window'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(x, remove_stopwords=False):\n",
    "    '''\n",
    "    Function to clear the html tags, then remove non-letters characters, \n",
    "    lowercase, remove stopwords, then join the resulting as a full text\n",
    "    '''\n",
    "    # Clean html tags\n",
    "    review = BeautifulSoup(x, \"html.parser\").get_text() \n",
    "    # Remove non letters     \n",
    "    letters = re.sub(\"[^a-zA-Z]\", \" \", review) \n",
    "    # Lowercase and split to words\n",
    "    words = letters.lower().split()                             \n",
    "    # It appears that searching in a set in Python is much more efficient than searching in a list\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # Optionally remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "    return words \n",
    "\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Function that splits reviews into sentences\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False ):\n",
    "    '''Function to split a review into parsed sentences. Returns a \n",
    "    list of sentences, where each sentence is a list of words\n",
    "    '''\n",
    "    \n",
    "    # Split into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    # Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(process(raw_sentence, remove_stopwords))\n",
    "            \n",
    "    # Return the list of sentences (each sentence is a list of words, so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from training set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n",
      "C:\\Users\\Peter martigny\\Anaconda3\\envs\\py35\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You shouldprobably open this file and pass the filehandle intoBeautiful Soup.\n",
      "  'Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547.5561151504517\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to the whole dataset\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences from training set\")\n",
    "for review in data[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "      \n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
